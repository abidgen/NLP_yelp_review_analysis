{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "# NLP: Analyzing Review Text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Unstructured data makes up the vast majority of data.  This is a basic intro to handling unstructured data.  Our objective is to be able to extract the sentiment (positive or negative) and gain insight from review text.  We will do this from Yelp review data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Metrics and scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "The first two questions task you to build models, of increasing complexity, to predict the rating of a review from its text. The grader uses a test set to evaluate your model's performance against our reference solution, using the $R^2$ score. It **is** possible to receive a score greater than one, indicating that you've beaten our reference model. We compare our model's score on a test set to your score on the same test set. See how high you can go!\n",
    "\n",
    "The final two questions asks only for the result of a calculation, and your results will be compared directly to those of a reference solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Download and parse the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "The training data are a series of JSON objects, in a Gzipped file. Python supports Gzipped files natively: [`gzip.open`](https://docs.python.org/3/library/gzip.html) has the same interface as `open`, but handles `.gz` files automatically.\n",
    "\n",
    "The built-in `json` package has a `loads` function that converts a JSON string into a Python dictionary. We could call that once for each row of the file. [`ujson`](http://docs.micropython.org/en/latest/library/ujson.html) has the same interface as the built-in `json` package, but is *substantially* faster (at the cost of non-robust handling of malformed JSON). We will use that inside a list comprehension to get a list of dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import ujson as json\n",
    "## https://mailuc-my.sharepoint.com/:u:/r/personal/rezaal_mail_uc_edu/Documents/yelp_train_academic_dataset_review_reduced.json.gz?csf=1&web=1&e=e9qC5e\n",
    "with gzip.open('yelp_train_academic_dataset_review_reduced.json.gz') as f:\n",
    "    data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "The scikit-learn API requires that we keep labels (in this case, the star ratings) and features in separate data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts = [row['text'] for row in data]\n",
    "# texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253272"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stars = [row['stars'] for row in data]\n",
    "len(stars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "# Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 1: bag_of_words_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Build a linear model predicting the star rating based on the text reviews. Apply the bag-of-words model using the [`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) to produce a feature matrix giving the counts of each word in each review.\n",
    "\n",
    "**Hints**:\n",
    "1. You will need to extract the review text from the raw input data, a list of dictionaries. You can take a similar approach you took in the `ml` miniproject by first converting the data into a pandas data frame and then using [`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html?highlight=columntransformer#sklearn.compose.ColumnTransformer) or you can build a custom transform to extract the text. Either way, remember that the `CountVectorizer` accepts as input to its `transform` method a 1D array of text.\n",
    "\n",
    "1. Try choosing different values for `min_df` (minimum document frequency cutoff) and `max_df` in `CountVectorizer`. Setting `min_df` to zero admits rare words which might only appear once in the entire corpus.  This is both prone to overfitting and makes your data unmanageably large.  Don't forget to use cross-validation to select the right value.\n",
    "\n",
    "1. Try using [`LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression) or [`Ridge`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html?highlight=ridge#sklearn.linear_model.Ridge). There is also [`RidgeCV`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html?highlight=ridge#sklearn.linear_model.RidgeCV) which has built-in leave-on-out cross-validation. If the memory footprint is too big, try switching to [Stochastic Gradient Descent](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor). Don't forget to search for the optimal value of the regularization parameter. How do the regularization parameter `alpha` and the values of `min_df` and `max_df` from `CountVectorizer` change the answer?\n",
    "\n",
    "1. You will likely pick up several hyperparameters between the vectorization step and the regularization of the predictor. While it is more strictly correct to do a grid search over all of them at once, this can take a long time. Quite often, doing a grid search over a single hyperparameter at a time can produce similar results.  Alternatively, the grid search may be done over a smaller subset of the data, as long as it is representative of the whole.\n",
    "\n",
    "1. Finally, assemble a pipeline that will transform the data from list of dictionaries all the way to predictions.  This will allow you to submit the model's `predict` method to the grader for scoring as the test set used by the grader is a list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class extract_text(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return pd.Series([j['text'] for j in X])\n",
    "\n",
    "\n",
    "# class ToDataFrame(BaseEstimator, TransformerMixin):\n",
    "#     def fit(self, X, y=None):\n",
    "#         # This transformer doesn't need to learn anything about the data,\n",
    "#         # so it can just return self without any further processing\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         # Return a pandas data frame from X\n",
    "#         return pd.DataFrame(X)\n",
    "\n",
    "\n",
    "# to_data_frame = ToDataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg = extract_text().fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         I don't know what Dr. Goldberg was like before...\n",
       "1         If you like lot lizards, you'll love the Pine ...\n",
       "2         Only went here once about a year and a half ag...\n",
       "3         Ate a Saturday morning breakfast at the Pine C...\n",
       "4         This is definitely not your usual truck stop. ...\n",
       "                                ...                        \n",
       "253267    What a horrible time yesterday... shYEAH - and...\n",
       "253268    My new favorite restaurant.  They have 22 diff...\n",
       "253269    GreAt food awesome service . The best fish in ...\n",
       "253270    I love this place! I think the staff struggle ...\n",
       "253271    I visit here once or twice a month. Just to ge...\n",
       "Length: 253272, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.compose import ColumnTransformer\n",
    "# text_selector = ColumnTransformer([\n",
    "#     ('text_extract','passthrough' , ['text'])\n",
    "# ])\n",
    "\n",
    "\n",
    "\n",
    "# # ColumnTransformer([('something', SomeTransformer(), ['column'])])\n",
    "\n",
    "\n",
    "# text_selector.fit_transform(gg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "# init stemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def text_preprocessor(text):\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"\\\\W\", \" \", text)  # remove special chars\n",
    "#     text=re.sub(\"\\\\s+(in|the|all|for|and|on)\\\\s+\",\" _connector_ \",text) # normalize certain words\n",
    "\n",
    "    # stem words\n",
    "    words = re.split(\"\\\\s+\", text)\n",
    "    stemmed_words = [porter_stemmer.stem(word=word) for word in words]\n",
    "    return ' '.join(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(253272, 12398)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "\n",
    "STOP_WORDS = STOP_WORDS.difference({'he', 'his', 'her', 'hers'})\n",
    "STOP_WORDS = STOP_WORDS.union({'ll', 've', 'abov', 'afterward',\n",
    "                               'alon', 'alreadi', 'alway', 'ani',\n",
    "                               'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'doe', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi',\n",
    "                              'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'quit', 'realli', 'regard', 'seriou', 'sever', 'sinc', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'use', 'variou', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv', 'anywh', 'becau', 'el', 'elsewh', 'everywh', 'ind', 'otherwi', 'plea', 'somewh'})\n",
    "\n",
    "count_vect = CountVectorizer(\n",
    "    preprocessor=text_preprocessor,\n",
    "    stop_words=STOP_WORDS,\n",
    "    min_df=0.0001,\n",
    "    max_df=0.9)\n",
    "\n",
    "# parameters = {'max_df': np.linspace(0.0, 1.0, 11),\n",
    "#               'min_df': np.linspace(0.0, 1.0, 11)}\n",
    "\n",
    "# count_vect_grid = GridSearchCV(count_vect,\n",
    "#                                parameters,\n",
    "#                                cv=KFold(n_splits=5,\n",
    "#                                         shuffle=True,\n",
    "#                                         random_state=1169),\n",
    "#                                verbose=3)\n",
    "\n",
    "\n",
    "kk = count_vect.fit_transform(gg)\n",
    "\n",
    "kk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV 1/5] END .........................alpha=1e-10;, score=nan total time=   0.0s\n",
      "[CV 2/5] END .........................alpha=1e-10;, score=nan total time=   0.0s\n",
      "[CV 3/5] END .........................alpha=1e-10;, score=nan total time=   0.0s\n",
      "[CV 4/5] END .........................alpha=1e-10;, score=nan total time=   0.0s\n",
      "[CV 5/5] END .........................alpha=1e-10;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ........alpha=1.6681005372000556e-09;, score=nan total time=   0.0s\n",
      "[CV 2/5] END ........alpha=1.6681005372000556e-09;, score=nan total time=   0.0s\n",
      "[CV 3/5] END ........alpha=1.6681005372000556e-09;, score=nan total time=   0.0s\n",
      "[CV 4/5] END ........alpha=1.6681005372000556e-09;, score=nan total time=   0.0s\n",
      "[CV 5/5] END ........alpha=1.6681005372000556e-09;, score=nan total time=   0.0s\n",
      "[CV 1/5] END .........alpha=2.782559402207126e-08;, score=nan total time=   0.0s\n",
      "[CV 2/5] END .........alpha=2.782559402207126e-08;, score=nan total time=   0.0s\n",
      "[CV 3/5] END .........alpha=2.782559402207126e-08;, score=nan total time=   0.0s\n",
      "[CV 4/5] END .........alpha=2.782559402207126e-08;, score=nan total time=   0.0s\n",
      "[CV 5/5] END .........alpha=2.782559402207126e-08;, score=nan total time=   0.0s\n",
      "[CV 1/5] END .........alpha=4.641588833612782e-07;, score=nan total time=   0.0s\n",
      "[CV 2/5] END .........alpha=4.641588833612782e-07;, score=nan total time=   0.0s\n",
      "[CV 3/5] END .........alpha=4.641588833612782e-07;, score=nan total time=   0.0s\n",
      "[CV 4/5] END .........alpha=4.641588833612782e-07;, score=nan total time=   0.0s\n",
      "[CV 5/5] END .........alpha=4.641588833612782e-07;, score=nan total time=   0.0s\n",
      "[CV 1/5] END .........alpha=7.742636826811277e-06;, score=nan total time=   0.0s\n",
      "[CV 2/5] END .........alpha=7.742636826811277e-06;, score=nan total time=   0.0s\n",
      "[CV 3/5] END .........alpha=7.742636826811277e-06;, score=nan total time=   0.0s\n",
      "[CV 4/5] END .........alpha=7.742636826811277e-06;, score=nan total time=   0.0s\n",
      "[CV 5/5] END .........alpha=7.742636826811277e-06;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ........alpha=0.00012915496650148855;, score=nan total time=   0.0s\n",
      "[CV 2/5] END ........alpha=0.00012915496650148855;, score=nan total time=   0.0s\n",
      "[CV 3/5] END ........alpha=0.00012915496650148855;, score=nan total time=   0.0s\n",
      "[CV 4/5] END ........alpha=0.00012915496650148855;, score=nan total time=   0.0s\n",
      "[CV 5/5] END ........alpha=0.00012915496650148855;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ..........alpha=0.002154434690031887;, score=nan total time=   0.0s\n",
      "[CV 2/5] END ..........alpha=0.002154434690031887;, score=nan total time=   0.0s\n",
      "[CV 3/5] END ..........alpha=0.002154434690031887;, score=nan total time=   0.0s\n",
      "[CV 4/5] END ..........alpha=0.002154434690031887;, score=nan total time=   0.0s\n",
      "[CV 5/5] END ..........alpha=0.002154434690031887;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ............alpha=0.0359381366380464;, score=nan total time=   0.0s\n",
      "[CV 2/5] END ............alpha=0.0359381366380464;, score=nan total time=   0.0s\n",
      "[CV 3/5] END ............alpha=0.0359381366380464;, score=nan total time=   0.0s\n",
      "[CV 4/5] END ............alpha=0.0359381366380464;, score=nan total time=   0.0s\n",
      "[CV 5/5] END ............alpha=0.0359381366380464;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ............alpha=0.5994842503189421;, score=nan total time=   0.0s\n",
      "[CV 2/5] END ............alpha=0.5994842503189421;, score=nan total time=   0.0s\n",
      "[CV 3/5] END ............alpha=0.5994842503189421;, score=nan total time=   0.0s\n",
      "[CV 4/5] END ............alpha=0.5994842503189421;, score=nan total time=   0.0s\n",
      "[CV 5/5] END ............alpha=0.5994842503189421;, score=nan total time=   0.0s\n",
      "[CV 1/5] END ..........................alpha=10.0;, score=nan total time=   0.0s\n",
      "[CV 2/5] END ..........................alpha=10.0;, score=nan total time=   0.0s\n",
      "[CV 3/5] END ..........................alpha=10.0;, score=nan total time=   0.0s\n",
      "[CV 4/5] END ..........................alpha=10.0;, score=nan total time=   0.0s\n",
      "[CV 5/5] END ..........................alpha=10.0;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/metrics/_regression.py:781: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/opt/conda/envs/data3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Ridge(),\n",
       "             param_grid={'alpha': array([1.00000000e-10, 1.66810054e-09, 2.78255940e-08, 4.64158883e-07,\n",
       "       7.74263683e-06, 1.29154967e-04, 2.15443469e-03, 3.59381366e-02,\n",
       "       5.99484250e-01, 1.00000000e+01])},\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge , RidgeCV\n",
    "n_alphas = 10\n",
    "alphas = np.logspace(-10, 1, n_alphas)\n",
    "\n",
    "parameters = {'alpha': alphas}\n",
    "ridge_ = Ridge()\n",
    "ridge_grid = GridSearchCV(ridge_, parameters, verbose=3)\n",
    "\n",
    "# ridge_grid.fit(kk,stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 4, 3, 3]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stars[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 4., 4., 3., 3.])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_grid.predict(kk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "bag_of_words_vectorizer = Pipeline([\n",
    "#     ('to_df', to_data_frame),\n",
    "#     ('text_selector', text_selector),\n",
    "    ('extract_text',extract_text()),\n",
    "    ('count_vect', count_vect),\n",
    "    ('ridge_grid', ridge_grid)\n",
    "], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ...... (step 1 of 3) Processing extract_text, total=   0.1s\n",
      "[Pipeline] ........ (step 2 of 3) Processing count_vect, total= 5.9min\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV 1/5] END .......................alpha=1e-10;, score=0.449 total time=  12.2s\n",
      "[CV 2/5] END .......................alpha=1e-10;, score=0.414 total time=  11.5s\n",
      "[CV 3/5] END .......................alpha=1e-10;, score=0.434 total time=  11.6s\n",
      "[CV 4/5] END .......................alpha=1e-10;, score=0.456 total time=  12.0s\n",
      "[CV 5/5] END .......................alpha=1e-10;, score=0.466 total time=  12.2s\n",
      "[CV 1/5] END ......alpha=1.6681005372000556e-09;, score=0.449 total time=  12.6s\n",
      "[CV 2/5] END ......alpha=1.6681005372000556e-09;, score=0.414 total time=  11.5s\n",
      "[CV 3/5] END ......alpha=1.6681005372000556e-09;, score=0.434 total time=  12.1s\n",
      "[CV 4/5] END ......alpha=1.6681005372000556e-09;, score=0.456 total time=  12.0s\n",
      "[CV 5/5] END ......alpha=1.6681005372000556e-09;, score=0.466 total time=  12.2s\n",
      "[CV 1/5] END .......alpha=2.782559402207126e-08;, score=0.449 total time=  12.5s\n",
      "[CV 2/5] END .......alpha=2.782559402207126e-08;, score=0.414 total time=  11.7s\n",
      "[CV 3/5] END .......alpha=2.782559402207126e-08;, score=0.434 total time=  12.2s\n",
      "[CV 4/5] END .......alpha=2.782559402207126e-08;, score=0.456 total time=  12.0s\n",
      "[CV 5/5] END .......alpha=2.782559402207126e-08;, score=0.466 total time=  12.1s\n",
      "[CV 1/5] END .......alpha=4.641588833612782e-07;, score=0.449 total time=  12.3s\n",
      "[CV 2/5] END .......alpha=4.641588833612782e-07;, score=0.414 total time=  11.2s\n",
      "[CV 3/5] END .......alpha=4.641588833612782e-07;, score=0.434 total time=  11.9s\n",
      "[CV 4/5] END .......alpha=4.641588833612782e-07;, score=0.456 total time=  12.1s\n",
      "[CV 5/5] END .......alpha=4.641588833612782e-07;, score=0.466 total time=  12.4s\n",
      "[CV 1/5] END .......alpha=7.742636826811277e-06;, score=0.449 total time=  12.4s\n",
      "[CV 2/5] END .......alpha=7.742636826811277e-06;, score=0.414 total time=  12.1s\n",
      "[CV 3/5] END .......alpha=7.742636826811277e-06;, score=0.434 total time=  12.1s\n",
      "[CV 4/5] END .......alpha=7.742636826811277e-06;, score=0.456 total time=  11.4s\n",
      "[CV 5/5] END .......alpha=7.742636826811277e-06;, score=0.466 total time=  12.2s\n",
      "[CV 1/5] END ......alpha=0.00012915496650148855;, score=0.449 total time=  12.4s\n",
      "[CV 2/5] END ......alpha=0.00012915496650148855;, score=0.414 total time=  11.2s\n",
      "[CV 3/5] END ......alpha=0.00012915496650148855;, score=0.434 total time=  11.9s\n",
      "[CV 4/5] END ......alpha=0.00012915496650148855;, score=0.456 total time=  12.0s\n",
      "[CV 5/5] END ......alpha=0.00012915496650148855;, score=0.466 total time=  12.2s\n",
      "[CV 1/5] END ........alpha=0.002154434690031887;, score=0.449 total time=  12.3s\n",
      "[CV 2/5] END ........alpha=0.002154434690031887;, score=0.414 total time=  11.1s\n",
      "[CV 3/5] END ........alpha=0.002154434690031887;, score=0.434 total time=  12.0s\n",
      "[CV 4/5] END ........alpha=0.002154434690031887;, score=0.456 total time=  11.9s\n",
      "[CV 5/5] END ........alpha=0.002154434690031887;, score=0.466 total time=  12.0s\n",
      "[CV 1/5] END ..........alpha=0.0359381366380464;, score=0.450 total time=  11.9s\n",
      "[CV 2/5] END ..........alpha=0.0359381366380464;, score=0.414 total time=  11.2s\n",
      "[CV 3/5] END ..........alpha=0.0359381366380464;, score=0.434 total time=  12.1s\n",
      "[CV 4/5] END ..........alpha=0.0359381366380464;, score=0.456 total time=  11.7s\n",
      "[CV 5/5] END ..........alpha=0.0359381366380464;, score=0.466 total time=  11.9s\n",
      "[CV 1/5] END ..........alpha=0.5994842503189421;, score=0.450 total time=  12.0s\n",
      "[CV 2/5] END ..........alpha=0.5994842503189421;, score=0.416 total time=  11.2s\n",
      "[CV 3/5] END ..........alpha=0.5994842503189421;, score=0.435 total time=  12.0s\n",
      "[CV 4/5] END ..........alpha=0.5994842503189421;, score=0.457 total time=  12.0s\n",
      "[CV 5/5] END ..........alpha=0.5994842503189421;, score=0.467 total time=  11.7s\n",
      "[CV 1/5] END ........................alpha=10.0;, score=0.461 total time=  10.9s\n",
      "[CV 2/5] END ........................alpha=10.0;, score=0.430 total time=  10.2s\n",
      "[CV 3/5] END ........................alpha=10.0;, score=0.446 total time=  10.5s\n",
      "[CV 4/5] END ........................alpha=10.0;, score=0.467 total time=  10.2s\n",
      "[CV 5/5] END ........................alpha=10.0;, score=0.479 total time=  10.4s\n",
      "[Pipeline] ........ (step 3 of 3) Processing ridge_grid, total=10.0min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('extract_text', extract_text()),\n",
       "                ('count_vect',\n",
       "                 CountVectorizer(max_df=0.9, min_df=0.0001,\n",
       "                                 preprocessor=<function text_preprocessor at 0x7f2429a88b80>,\n",
       "                                 stop_words={\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\",\n",
       "                                             \"'ve\", 'a', 'about', 'above',\n",
       "                                             'across', 'after', 'afterwards',\n",
       "                                             'again', 'against', 'all',\n",
       "                                             'almost', 'alone', 'along',\n",
       "                                             'already', 'also', 'although',\n",
       "                                             'always', 'am', 'among', 'amongst',\n",
       "                                             'amount', 'an', 'and', 'another',\n",
       "                                             'any', ...})),\n",
       "                ('ridge_grid',\n",
       "                 GridSearchCV(estimator=Ridge(),\n",
       "                              param_grid={'alpha': array([1.00000000e-10, 1.66810054e-09, 2.78255940e-08, 4.64158883e-07,\n",
       "       7.74263683e-06, 1.29154967e-04, 2.15443469e-03, 3.59381366e-02,\n",
       "       5.99484250e-01, 1.00000000e+01])},\n",
       "                              verbose=3))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words_model = bag_of_words_vectorizer\n",
    "\n",
    "bag_of_words_model.fit(data, stars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 2: bigram_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "In a bigram model, we'll consider both single words and pairs of consecutive words that appear. This is going to be a much higher-dimensional problem so you should be careful about overfitting. You should also use a vectorizer that applies some sort of normalization, e.g., the `TfidfVectorizer` or a word count vectorizer combined with `TfidfTransformer`.\n",
    "\n",
    "Sometimes, reducing the dimension can be useful. If you're using the `TfidfVectorizer`, you can change the `max_features` hyperparameter to reduce the size of the resulting vocabulary. For `HashingVectorizer`, you can adjust the size of the feature matrix through `n_features`.\n",
    "\n",
    "**A side note on multi-stage model evaluation:** When your model consists of a pipeline with several stages, it can be worthwhile to evaluate which parts of the pipeline have the greatest impact on the overall accuracy (or other metric) of the model. This allows you to focus your efforts on improving the important algorithms, and leaving the rest \"good enough\".\n",
    "\n",
    "One way to accomplish this is through ceiling analysis, which can be useful when you have a training set with ground truth values at each stage. Let's say you're training a model to extract image captions from websites and return a list of names that were in the caption. Your overall accuracy at some point reaches 70%. You can try manually giving the model what you know are the correct image captions from the training set, and see how the accuracy improves (maybe up to 75%). Alternatively, giving the model the perfect name parsing for each caption increases accuracy to 90%. This indicates that the name parsing is a much more promising target for further work, and the caption extraction is a relatively smaller factor in the overall performance.\n",
    "\n",
    "If you don't know the right answers at different stages of the pipeline, you can still evaluate how important different parts of the model are to its performance by changing or removing certain steps while keeping everything else constant. You might try this kind of analysis to determine how important adding stopwords and stemming to your NLP model actually is, and how that importance changes with parameters like the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class extract_text(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return pd.Series([j['text'] for j in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg = extract_text().fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\") \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "def tokenize_lemma(text):\n",
    "    return [w.lemma_.lower() for w in nlp(text)]\n",
    "\n",
    "\n",
    "stop_words_lemma = set(tokenize_lemma(' '.join(sorted(STOP_WORDS))))\n",
    "\n",
    "\n",
    "ng_stem_tfidf = TfidfVectorizer(max_features=5000, \n",
    "                                stop_words=stop_words_lemma,\n",
    "#                                 tokenizer=tokenize_lemma,\n",
    "#                                 token_pattern=None, # Is ignored, since tokenizer is specified\n",
    "                                ngram_range = (1,2)\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge , RidgeCV\n",
    "n_alphas = 10\n",
    "alphas = np.logspace(-10, 1, n_alphas)\n",
    "\n",
    "parameters = {'alpha': alphas}\n",
    "ridge_ = Ridge()\n",
    "ridge_grid = GridSearchCV(ridge_, parameters, verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "bigram_model = Pipeline([\n",
    "#     ('to_df', to_data_frame),\n",
    "#     ('text_selector', text_selector),\n",
    "    ('extract_text',extract_text()),\n",
    "    ('ng_stem_tfidf', ng_stem_tfidf),\n",
    "    ('ridge_grid', ridge_grid)\n",
    "], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ...... (step 1 of 3) Processing extract_text, total=   0.1s\n",
      "[Pipeline] ..... (step 2 of 3) Processing ng_stem_tfidf, total=  49.5s\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV 1/5] END .......................alpha=1e-10;, score=0.592 total time=   1.3s\n",
      "[CV 2/5] END .......................alpha=1e-10;, score=0.571 total time=   1.2s\n",
      "[CV 3/5] END .......................alpha=1e-10;, score=0.583 total time=   1.2s\n",
      "[CV 4/5] END .......................alpha=1e-10;, score=0.606 total time=   1.2s\n",
      "[CV 5/5] END .......................alpha=1e-10;, score=0.628 total time=   1.2s\n",
      "[CV 1/5] END ......alpha=1.6681005372000556e-09;, score=0.592 total time=   1.3s\n",
      "[CV 2/5] END ......alpha=1.6681005372000556e-09;, score=0.571 total time=   1.3s\n",
      "[CV 3/5] END ......alpha=1.6681005372000556e-09;, score=0.583 total time=   1.2s\n",
      "[CV 4/5] END ......alpha=1.6681005372000556e-09;, score=0.606 total time=   1.2s\n",
      "[CV 5/5] END ......alpha=1.6681005372000556e-09;, score=0.628 total time=   1.2s\n",
      "[CV 1/5] END .......alpha=2.782559402207126e-08;, score=0.592 total time=   1.3s\n",
      "[CV 2/5] END .......alpha=2.782559402207126e-08;, score=0.571 total time=   1.2s\n",
      "[CV 3/5] END .......alpha=2.782559402207126e-08;, score=0.583 total time=   1.2s\n",
      "[CV 4/5] END .......alpha=2.782559402207126e-08;, score=0.606 total time=   1.2s\n",
      "[CV 5/5] END .......alpha=2.782559402207126e-08;, score=0.628 total time=   1.2s\n",
      "[CV 1/5] END .......alpha=4.641588833612782e-07;, score=0.592 total time=   1.3s\n",
      "[CV 2/5] END .......alpha=4.641588833612782e-07;, score=0.571 total time=   1.3s\n",
      "[CV 3/5] END .......alpha=4.641588833612782e-07;, score=0.583 total time=   1.2s\n",
      "[CV 4/5] END .......alpha=4.641588833612782e-07;, score=0.606 total time=   1.2s\n",
      "[CV 5/5] END .......alpha=4.641588833612782e-07;, score=0.628 total time=   1.2s\n",
      "[CV 1/5] END .......alpha=7.742636826811277e-06;, score=0.592 total time=   1.3s\n",
      "[CV 2/5] END .......alpha=7.742636826811277e-06;, score=0.571 total time=   1.2s\n",
      "[CV 3/5] END .......alpha=7.742636826811277e-06;, score=0.583 total time=   1.2s\n",
      "[CV 4/5] END .......alpha=7.742636826811277e-06;, score=0.606 total time=   1.2s\n",
      "[CV 5/5] END .......alpha=7.742636826811277e-06;, score=0.628 total time=   1.2s\n",
      "[CV 1/5] END ......alpha=0.00012915496650148855;, score=0.592 total time=   1.3s\n",
      "[CV 2/5] END ......alpha=0.00012915496650148855;, score=0.571 total time=   1.2s\n",
      "[CV 3/5] END ......alpha=0.00012915496650148855;, score=0.583 total time=   1.2s\n",
      "[CV 4/5] END ......alpha=0.00012915496650148855;, score=0.606 total time=   1.2s\n",
      "[CV 5/5] END ......alpha=0.00012915496650148855;, score=0.628 total time=   1.2s\n",
      "[CV 1/5] END ........alpha=0.002154434690031887;, score=0.592 total time=   1.3s\n",
      "[CV 2/5] END ........alpha=0.002154434690031887;, score=0.571 total time=   1.2s\n",
      "[CV 3/5] END ........alpha=0.002154434690031887;, score=0.583 total time=   1.2s\n",
      "[CV 4/5] END ........alpha=0.002154434690031887;, score=0.606 total time=   1.3s\n",
      "[CV 5/5] END ........alpha=0.002154434690031887;, score=0.628 total time=   1.3s\n",
      "[CV 1/5] END ..........alpha=0.0359381366380464;, score=0.592 total time=   1.3s\n",
      "[CV 2/5] END ..........alpha=0.0359381366380464;, score=0.571 total time=   1.3s\n",
      "[CV 3/5] END ..........alpha=0.0359381366380464;, score=0.583 total time=   1.2s\n",
      "[CV 4/5] END ..........alpha=0.0359381366380464;, score=0.606 total time=   1.2s\n",
      "[CV 5/5] END ..........alpha=0.0359381366380464;, score=0.628 total time=   1.2s\n",
      "[CV 1/5] END ..........alpha=0.5994842503189421;, score=0.592 total time=   1.1s\n",
      "[CV 2/5] END ..........alpha=0.5994842503189421;, score=0.572 total time=   1.1s\n",
      "[CV 3/5] END ..........alpha=0.5994842503189421;, score=0.584 total time=   1.1s\n",
      "[CV 4/5] END ..........alpha=0.5994842503189421;, score=0.607 total time=   1.1s\n",
      "[CV 5/5] END ..........alpha=0.5994842503189421;, score=0.629 total time=   1.1s\n",
      "[CV 1/5] END ........................alpha=10.0;, score=0.590 total time=   0.8s\n",
      "[CV 2/5] END ........................alpha=10.0;, score=0.575 total time=   0.8s\n",
      "[CV 3/5] END ........................alpha=10.0;, score=0.584 total time=   0.8s\n",
      "[CV 4/5] END ........................alpha=10.0;, score=0.605 total time=   0.8s\n",
      "[CV 5/5] END ........................alpha=10.0;, score=0.627 total time=   0.9s\n",
      "[Pipeline] ........ (step 3 of 3) Processing ridge_grid, total= 1.0min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('extract_text', extract_text()),\n",
       "                ('ng_stem_tfidf',\n",
       "                 TfidfVectorizer(max_features=5000, ngram_range=(1, 2),\n",
       "                                 stop_words={\"'\", \"'ll\", 'a', 'about', 'above',\n",
       "                                             'across', 'after', 'afterwards',\n",
       "                                             'again', 'against', 'all',\n",
       "                                             'almost', 'alone', 'along',\n",
       "                                             'already', 'also', 'although',\n",
       "                                             'always', 'among', 'amongst',\n",
       "                                             'amount', 'an', 'and', 'another',\n",
       "                                             'any', 'anyhow', 'anyone',\n",
       "                                             'anything', 'anyway', 'anywhere', ...})),\n",
       "                ('ridge_grid',\n",
       "                 GridSearchCV(estimator=Ridge(),\n",
       "                              param_grid={'alpha': array([1.00000000e-10, 1.66810054e-09, 2.78255940e-08, 4.64158883e-07,\n",
       "       7.74263683e-06, 1.29154967e-04, 2.15443469e-03, 3.59381366e-02,\n",
       "       5.99484250e-01, 1.00000000e+01])},\n",
       "                              verbose=3))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bigram_model = ...\n",
    "\n",
    "bigram_model.fit(data, stars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 3: word_polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Let's consider a different approach and try to derive some insight from our analysis.  \n",
    "\n",
    "We want to determine the most \"polarizing words\" in the corpus of reviews.  In other words, we want to identify words that strongly signal a review is either positive or negative.  For example, we understand that a word like \"terrible\" will most likely appear in negative rather than positive reviews.  \n",
    "\n",
    "During training, the [naive Bayes model](https://scikit-learn.org/stable/modules/naive_bayes.html#) calculates probabilities such as $Pr(\\textrm{terrible}\\ |\\ \\textrm{negative}),$ the probability that the word \"terrible\" appears in the review text, given that the review is negative.  Using these probabilities, we can define a **polarity score** for each word $w$,\n",
    "\n",
    "$$\\textrm{polarity}(w) = \\log\\left(\\frac{Pr(w\\ |\\ \\textrm{positive})}{Pr(w\\ |\\ \\textrm{negative})}\\right).$$\n",
    "\n",
    "Polarity analysis is an example where a simpler model (naive Bayes) offers more explicability than more complicated models.  Aside from this, naive Bayes models are easy to train, the training process is parallelizable, and these models lend themselves well to online learning.  Given enough training data, naive Bayes models have performed well in NLP applications such as spam filtering.  \n",
    "\n",
    "For this problem, you are asked to determine the top 25 most positive polar words and the 25 most negative polar words.  For this analysis, you should:\n",
    "\n",
    "1.  **Filter** the collection of reviews you were using above to **only keep** the one-star and five-star reviews. Since these are the \"most polar\" reviews, it should give us the most polarizing words.   \n",
    "1.  Use the naive Bayes model, `MultinomialNB`.  \n",
    "1.  Use TF-IDF weighting.\n",
    "1.  Remove stop words.\n",
    "1.  As mentioned, generate a (Python) list with most positive (25 words) and most negative (25 words) polar words.  \n",
    "\n",
    "A naive Bayes model (after training) stores the log of the probabilities in an attribute of the model.  It is a `numpy` array of shape (number of classes, number of features).  You will need the mapping between feature indices to words to find the most polarizing words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class extract_text(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        filtered_data = [{'text':j['text'], 'stars':j['stars']} for j in X if j['stars']==1 or j['stars']==5] \n",
    "        filtered_texts = pd.Series([i['text'] for i in filtered_data])\n",
    "        filtered_stars = pd.Series([i['stars'] for i in filtered_data])\n",
    "        return filtered_data, filtered_texts , filtered_stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "polar_data,polar_texts, polar_stars = extract_text().fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2893820340378809"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(['great' in i for i in polar_texts])/len(polar_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We're only keeping the one and five star reviews\n",
    "grader.check(len(polar_data) == 116576)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\") \n",
    "\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# def tokenize_lemma(text):\n",
    "#     return [w.lemma_.lower() for w in nlp(text)]\n",
    "\n",
    "\n",
    "# # stop_words_lemma = set(tokenize_lemma(' '.join(sorted(STOP_WORDS))))\n",
    "# STOP_WORDS = STOP_WORDS.union({'ll', 've'})\n",
    "\n",
    "tfidf = TfidfVectorizer(#max_features=5000, \n",
    "                                stop_words='english',\n",
    "#                                 tokenizer=tokenize_lemma,\n",
    "#                                 token_pattern=None, # Is ignored, since tokenizer is specified\n",
    "                                #ngram_range = (1,2),\n",
    "#                                 max_df = 0.13,\n",
    "#                                 smooth_idf= False\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kk = tfidf.fit_transform(polar_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88242"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "[CV 1/5] END .......................alpha=1e-10;, score=0.892 total time=   0.1s\n",
      "[CV 2/5] END .......................alpha=1e-10;, score=0.891 total time=   0.1s\n",
      "[CV 3/5] END .......................alpha=1e-10;, score=0.896 total time=   0.1s\n",
      "[CV 4/5] END .......................alpha=1e-10;, score=0.897 total time=   0.1s\n",
      "[CV 5/5] END .......................alpha=1e-10;, score=0.898 total time=   0.1s\n",
      "[CV 1/5] END ........alpha=1.67683293681101e-10;, score=0.893 total time=   0.1s\n",
      "[CV 2/5] END ........alpha=1.67683293681101e-10;, score=0.891 total time=   0.1s\n",
      "[CV 3/5] END ........alpha=1.67683293681101e-10;, score=0.897 total time=   0.1s\n",
      "[CV 4/5] END ........alpha=1.67683293681101e-10;, score=0.897 total time=   0.1s\n",
      "[CV 5/5] END ........alpha=1.67683293681101e-10;, score=0.898 total time=   0.1s\n",
      "[CV 1/5] END .......alpha=2.811768697974225e-10;, score=0.893 total time=   0.1s\n",
      "[CV 2/5] END .......alpha=2.811768697974225e-10;, score=0.892 total time=   0.1s\n",
      "[CV 3/5] END .......alpha=2.811768697974225e-10;, score=0.897 total time=   0.1s\n",
      "[CV 4/5] END .......alpha=2.811768697974225e-10;, score=0.897 total time=   0.1s\n",
      "[CV 5/5] END .......alpha=2.811768697974225e-10;, score=0.898 total time=   0.1s\n",
      "[CV 1/5] END ........alpha=4.71486636345739e-10;, score=0.893 total time=   0.1s\n",
      "[CV 2/5] END ........alpha=4.71486636345739e-10;, score=0.892 total time=   0.1s\n",
      "[CV 3/5] END ........alpha=4.71486636345739e-10;, score=0.897 total time=   0.1s\n",
      "[CV 4/5] END ........alpha=4.71486636345739e-10;, score=0.897 total time=   0.1s\n",
      "[CV 5/5] END ........alpha=4.71486636345739e-10;, score=0.898 total time=   0.1s\n",
      "[CV 1/5] END .......alpha=7.906043210907701e-10;, score=0.893 total time=   0.1s\n",
      "[CV 2/5] END .......alpha=7.906043210907701e-10;, score=0.893 total time=   0.1s\n",
      "[CV 3/5] END .......alpha=7.906043210907701e-10;, score=0.898 total time=   0.1s\n",
      "[CV 4/5] END .......alpha=7.906043210907701e-10;, score=0.898 total time=   0.1s\n",
      "[CV 5/5] END .......alpha=7.906043210907701e-10;, score=0.899 total time=   0.1s\n",
      "[CV 1/5] END ......alpha=1.3257113655901108e-09;, score=0.894 total time=   0.1s\n",
      "[CV 2/5] END ......alpha=1.3257113655901108e-09;, score=0.893 total time=   0.1s\n",
      "[CV 3/5] END ......alpha=1.3257113655901108e-09;, score=0.898 total time=   0.1s\n",
      "[CV 4/5] END ......alpha=1.3257113655901108e-09;, score=0.898 total time=   0.1s\n",
      "[CV 5/5] END ......alpha=1.3257113655901108e-09;, score=0.899 total time=   0.1s\n",
      "[CV 1/5] END .......alpha=2.222996482526191e-09;, score=0.894 total time=   0.1s\n",
      "[CV 2/5] END .......alpha=2.222996482526191e-09;, score=0.893 total time=   0.1s\n",
      "[CV 3/5] END .......alpha=2.222996482526191e-09;, score=0.899 total time=   0.1s\n",
      "[CV 4/5] END .......alpha=2.222996482526191e-09;, score=0.899 total time=   0.1s\n",
      "[CV 5/5] END .......alpha=2.222996482526191e-09;, score=0.900 total time=   0.1s\n",
      "[CV 1/5] END .......alpha=3.727593720314938e-09;, score=0.895 total time=   0.1s\n",
      "[CV 2/5] END .......alpha=3.727593720314938e-09;, score=0.894 total time=   0.1s\n",
      "[CV 3/5] END .......alpha=3.727593720314938e-09;, score=0.899 total time=   0.1s\n",
      "[CV 4/5] END .......alpha=3.727593720314938e-09;, score=0.899 total time=   0.1s\n",
      "[CV 5/5] END .......alpha=3.727593720314938e-09;, score=0.900 total time=   0.1s\n",
      "[CV 1/5] END .......alpha=6.250551925273976e-09;, score=0.895 total time=   0.1s\n",
      "[CV 2/5] END .......alpha=6.250551925273976e-09;, score=0.894 total time=   0.1s\n",
      "[CV 3/5] END .......alpha=6.250551925273976e-09;, score=0.899 total time=   0.1s\n",
      "[CV 4/5] END .......alpha=6.250551925273976e-09;, score=0.899 total time=   0.1s\n",
      "[CV 5/5] END .......alpha=6.250551925273976e-09;, score=0.900 total time=   0.1s\n",
      "[CV 1/5] END ......alpha=1.0481131341546852e-08;, score=0.895 total time=   0.1s\n",
      "[CV 2/5] END ......alpha=1.0481131341546852e-08;, score=0.895 total time=   0.1s\n",
      "[CV 3/5] END ......alpha=1.0481131341546852e-08;, score=0.900 total time=   0.1s\n",
      "[CV 4/5] END ......alpha=1.0481131341546852e-08;, score=0.900 total time=   0.1s\n",
      "[CV 5/5] END ......alpha=1.0481131341546852e-08;, score=0.901 total time=   0.1s\n",
      "[CV 1/5] END ......alpha=1.7575106248547893e-08;, score=0.896 total time=   0.1s\n",
      "[CV 2/5] END ......alpha=1.7575106248547893e-08;, score=0.895 total time=   0.1s\n",
      "[CV 3/5] END ......alpha=1.7575106248547893e-08;, score=0.900 total time=   0.1s\n",
      "[CV 4/5] END ......alpha=1.7575106248547893e-08;, score=0.900 total time=   0.1s\n",
      "[CV 5/5] END ......alpha=1.7575106248547893e-08;, score=0.901 total time=   0.1s\n",
      "[CV 1/5] END ......alpha=2.9470517025518096e-08;, score=0.897 total time=   0.1s\n",
      "[CV 2/5] END ......alpha=2.9470517025518096e-08;, score=0.896 total time=   0.1s\n",
      "[CV 3/5] END ......alpha=2.9470517025518096e-08;, score=0.901 total time=   0.1s\n",
      "[CV 4/5] END ......alpha=2.9470517025518096e-08;, score=0.901 total time=   0.1s\n",
      "[CV 5/5] END ......alpha=2.9470517025518096e-08;, score=0.902 total time=   0.1s\n",
      "[CV 1/5] END ......alpha=4.9417133613238385e-08;, score=0.897 total time=   0.1s\n",
      "[CV 2/5] END ......alpha=4.9417133613238385e-08;, score=0.896 total time=   0.1s\n",
      "[CV 3/5] END ......alpha=4.9417133613238385e-08;, score=0.902 total time=   0.1s\n",
      "[CV 4/5] END ......alpha=4.9417133613238385e-08;, score=0.902 total time=   0.1s\n",
      "[CV 5/5] END ......alpha=4.9417133613238385e-08;, score=0.902 total time=   0.1s\n",
      "[CV 1/5] END .......alpha=8.286427728546843e-08;, score=0.898 total time=   0.1s\n",
      "[CV 2/5] END .......alpha=8.286427728546843e-08;, score=0.897 total time=   0.1s\n",
      "[CV 3/5] END .......alpha=8.286427728546843e-08;, score=0.902 total time=   0.1s\n",
      "[CV 4/5] END .......alpha=8.286427728546843e-08;, score=0.903 total time=   0.1s\n",
      "[CV 5/5] END .......alpha=8.286427728546843e-08;, score=0.903 total time=   0.1s\n",
      "[CV 1/5] END .......alpha=1.389495494373136e-07;, score=0.899 total time=   0.1s\n",
      "[CV 2/5] END .......alpha=1.389495494373136e-07;, score=0.898 total time=   0.1s\n",
      "[CV 3/5] END .......alpha=1.389495494373136e-07;, score=0.903 total time=   0.1s\n",
      "[CV 4/5] END .......alpha=1.389495494373136e-07;, score=0.903 total time=   0.1s\n",
      "[CV 5/5] END .......alpha=1.389495494373136e-07;, score=0.903 total time=   0.1s\n",
      "[CV 1/5] END .......alpha=2.329951810515372e-07;, score=0.899 total time=   0.1s\n",
      "[CV 2/5] END .......alpha=2.329951810515372e-07;, score=0.899 total time=   0.1s\n",
      "[CV 3/5] END .......alpha=2.329951810515372e-07;, score=0.903 total time=   0.1s\n",
      "[CV 4/5] END .......alpha=2.329951810515372e-07;, score=0.904 total time=   0.1s\n",
      "[CV 5/5] END .......alpha=2.329951810515372e-07;, score=0.904 total time=   0.1s\n",
      "[CV 1/5] END .......alpha=3.906939937054621e-07;, score=0.901 total time=   0.1s\n",
      "[CV 2/5] END .......alpha=3.906939937054621e-07;, score=0.899 total time=   0.1s\n",
      "[CV 3/5] END .......alpha=3.906939937054621e-07;, score=0.904 total time=   0.1s\n",
      "[CV 4/5] END .......alpha=3.906939937054621e-07;, score=0.905 total time=   0.1s\n",
      "[CV 5/5] END .......alpha=3.906939937054621e-07;, score=0.904 total time=   0.1s\n",
      "[CV 1/5] END ........alpha=6.55128556859551e-07;, score=0.901 total time=   0.1s\n",
      "[CV 2/5] END ........alpha=6.55128556859551e-07;, score=0.900 total time=   0.1s\n",
      "[CV 3/5] END ........alpha=6.55128556859551e-07;, score=0.905 total time=   0.1s\n",
      "[CV 4/5] END ........alpha=6.55128556859551e-07;, score=0.906 total time=   0.1s\n",
      "[CV 5/5] END ........alpha=6.55128556859551e-07;, score=0.905 total time=   0.1s\n",
      "[CV 1/5] END ......alpha=1.0985411419875572e-06;, score=0.902 total time=   0.1s\n",
      "[CV 2/5] END ......alpha=1.0985411419875572e-06;, score=0.901 total time=   0.1s\n",
      "[CV 3/5] END ......alpha=1.0985411419875572e-06;, score=0.906 total time=   0.1s\n",
      "[CV 4/5] END ......alpha=1.0985411419875572e-06;, score=0.907 total time=   0.1s\n",
      "[CV 5/5] END ......alpha=1.0985411419875572e-06;, score=0.906 total time=   0.1s\n",
      "[CV 1/5] END ......alpha=1.8420699693267164e-06;, score=0.903 total time=   0.1s\n",
      "[CV 2/5] END ......alpha=1.8420699693267164e-06;, score=0.902 total time=   0.1s\n",
      "[CV 3/5] END ......alpha=1.8420699693267164e-06;, score=0.907 total time=   0.1s\n",
      "[CV 4/5] END ......alpha=1.8420699693267164e-06;, score=0.907 total time=   0.1s\n",
      "[CV 5/5] END ......alpha=1.8420699693267164e-06;, score=0.907 total time=   0.1s\n",
      "[CV 1/5] END ......alpha=3.0888435964774785e-06;, score=0.903 total time=   0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END ......alpha=3.0888435964774785e-06;, score=0.903 total time=   0.1s\n",
      "[CV 3/5] END ......alpha=3.0888435964774785e-06;, score=0.908 total time=   0.1s\n",
      "[CV 4/5] END ......alpha=3.0888435964774785e-06;, score=0.908 total time=   0.1s\n",
      "[CV 5/5] END ......alpha=3.0888435964774785e-06;, score=0.908 total time=   0.1s\n",
      "[CV 1/5] END .......alpha=5.179474679231212e-06;, score=0.904 total time=   0.1s\n",
      "[CV 2/5] END .......alpha=5.179474679231212e-06;, score=0.904 total time=   0.1s\n",
      "[CV 3/5] END .......alpha=5.179474679231212e-06;, score=0.908 total time=   0.1s\n",
      "[CV 4/5] END .......alpha=5.179474679231212e-06;, score=0.909 total time=   0.1s\n",
      "[CV 5/5] END .......alpha=5.179474679231212e-06;, score=0.909 total time=   0.1s\n",
      "[CV 1/5] END ........alpha=8.68511373751352e-06;, score=0.906 total time=   0.1s\n",
      "[CV 2/5] END ........alpha=8.68511373751352e-06;, score=0.905 total time=   0.1s\n",
      "[CV 3/5] END ........alpha=8.68511373751352e-06;, score=0.910 total time=   0.1s\n",
      "[CV 4/5] END ........alpha=8.68511373751352e-06;, score=0.910 total time=   0.1s\n",
      "[CV 5/5] END ........alpha=8.68511373751352e-06;, score=0.909 total time=   0.1s\n",
      "[CV 1/5] END ......alpha=1.4563484775012445e-05;, score=0.907 total time=   0.1s\n",
      "[CV 2/5] END ......alpha=1.4563484775012445e-05;, score=0.906 total time=   0.1s\n",
      "[CV 3/5] END ......alpha=1.4563484775012445e-05;, score=0.911 total time=   0.1s\n",
      "[CV 4/5] END ......alpha=1.4563484775012445e-05;, score=0.912 total time=   0.1s\n",
      "[CV 5/5] END ......alpha=1.4563484775012445e-05;, score=0.911 total time=   0.1s\n",
      "[CV 1/5] END ......alpha=2.4420530945486497e-05;, score=0.908 total time=   0.1s\n",
      "[CV 2/5] END ......alpha=2.4420530945486497e-05;, score=0.907 total time=   0.1s\n",
      "[CV 3/5] END ......alpha=2.4420530945486497e-05;, score=0.912 total time=   0.1s\n",
      "[CV 4/5] END ......alpha=2.4420530945486497e-05;, score=0.912 total time=   0.1s\n",
      "[CV 5/5] END ......alpha=2.4420530945486497e-05;, score=0.912 total time=   0.1s\n",
      "[CV 1/5] END .......alpha=4.094915062380427e-05;, score=0.909 total time=   0.1s\n",
      "[CV 2/5] END .......alpha=4.094915062380427e-05;, score=0.909 total time=   0.1s\n",
      "[CV 3/5] END .......alpha=4.094915062380427e-05;, score=0.914 total time=   0.1s\n",
      "[CV 4/5] END .......alpha=4.094915062380427e-05;, score=0.914 total time=   0.1s\n",
      "[CV 5/5] END .......alpha=4.094915062380427e-05;, score=0.913 total time=   0.1s\n",
      "[CV 1/5] END .......alpha=6.866488450042999e-05;, score=0.911 total time=   0.1s\n",
      "[CV 2/5] END .......alpha=6.866488450042999e-05;, score=0.910 total time=   0.1s\n",
      "[CV 3/5] END .......alpha=6.866488450042999e-05;, score=0.915 total time=   0.1s\n",
      "[CV 4/5] END .......alpha=6.866488450042999e-05;, score=0.915 total time=   0.1s\n",
      "[CV 5/5] END .......alpha=6.866488450042999e-05;, score=0.915 total time=   0.1s\n",
      "[CV 1/5] END ......alpha=0.00011513953993264481;, score=0.912 total time=   0.1s\n",
      "[CV 2/5] END ......alpha=0.00011513953993264481;, score=0.912 total time=   0.1s\n",
      "[CV 3/5] END ......alpha=0.00011513953993264481;, score=0.916 total time=   0.1s\n",
      "[CV 4/5] END ......alpha=0.00011513953993264481;, score=0.916 total time=   0.1s\n",
      "[CV 5/5] END ......alpha=0.00011513953993264481;, score=0.916 total time=   0.1s\n",
      "[CV 1/5] END ......alpha=0.00019306977288832496;, score=0.913 total time=   0.1s\n",
      "[CV 2/5] END ......alpha=0.00019306977288832496;, score=0.913 total time=   0.1s\n",
      "[CV 3/5] END ......alpha=0.00019306977288832496;, score=0.918 total time=   0.1s\n",
      "[CV 4/5] END ......alpha=0.00019306977288832496;, score=0.917 total time=   0.1s\n",
      "[CV 5/5] END ......alpha=0.00019306977288832496;, score=0.917 total time=   0.1s\n",
      "[CV 1/5] END ......alpha=0.00032374575428176466;, score=0.915 total time=   0.1s\n",
      "[CV 2/5] END ......alpha=0.00032374575428176466;, score=0.915 total time=   0.1s\n",
      "[CV 3/5] END ......alpha=0.00032374575428176466;, score=0.920 total time=   0.1s\n",
      "[CV 4/5] END ......alpha=0.00032374575428176466;, score=0.919 total time=   0.1s\n",
      "[CV 5/5] END ......alpha=0.00032374575428176466;, score=0.918 total time=   0.1s\n",
      "[CV 1/5] END .......alpha=0.0005428675439323859;, score=0.916 total time=   0.1s\n",
      "[CV 2/5] END .......alpha=0.0005428675439323859;, score=0.916 total time=   0.1s\n",
      "[CV 3/5] END .......alpha=0.0005428675439323859;, score=0.921 total time=   0.1s\n",
      "[CV 4/5] END .......alpha=0.0005428675439323859;, score=0.920 total time=   0.1s\n",
      "[CV 5/5] END .......alpha=0.0005428675439323859;, score=0.919 total time=   0.1s\n",
      "[CV 1/5] END .......alpha=0.0009102981779915227;, score=0.917 total time=   0.1s\n",
      "[CV 2/5] END .......alpha=0.0009102981779915227;, score=0.918 total time=   0.1s\n",
      "[CV 3/5] END .......alpha=0.0009102981779915227;, score=0.923 total time=   0.1s\n",
      "[CV 4/5] END .......alpha=0.0009102981779915227;, score=0.921 total time=   0.1s\n",
      "[CV 5/5] END .......alpha=0.0009102981779915227;, score=0.921 total time=   0.1s\n",
      "[CV 1/5] END .......alpha=0.0015264179671752333;, score=0.919 total time=   0.1s\n",
      "[CV 2/5] END .......alpha=0.0015264179671752333;, score=0.919 total time=   0.1s\n",
      "[CV 3/5] END .......alpha=0.0015264179671752333;, score=0.924 total time=   0.1s\n",
      "[CV 4/5] END .......alpha=0.0015264179671752333;, score=0.923 total time=   0.1s\n",
      "[CV 5/5] END .......alpha=0.0015264179671752333;, score=0.922 total time=   0.1s\n",
      "[CV 1/5] END .......alpha=0.0025595479226995332;, score=0.920 total time=   0.1s\n",
      "[CV 2/5] END .......alpha=0.0025595479226995332;, score=0.921 total time=   0.1s\n",
      "[CV 3/5] END .......alpha=0.0025595479226995332;, score=0.926 total time=   0.1s\n",
      "[CV 4/5] END .......alpha=0.0025595479226995332;, score=0.924 total time=   0.1s\n",
      "[CV 5/5] END .......alpha=0.0025595479226995332;, score=0.923 total time=   0.1s\n",
      "[CV 1/5] END ........alpha=0.004291934260128779;, score=0.922 total time=   0.1s\n",
      "[CV 2/5] END ........alpha=0.004291934260128779;, score=0.922 total time=   0.1s\n",
      "[CV 3/5] END ........alpha=0.004291934260128779;, score=0.927 total time=   0.1s\n",
      "[CV 4/5] END ........alpha=0.004291934260128779;, score=0.925 total time=   0.1s\n",
      "[CV 5/5] END ........alpha=0.004291934260128779;, score=0.925 total time=   0.1s\n",
      "[CV 1/5] END ........alpha=0.007196856730011514;, score=0.924 total time=   0.1s\n",
      "[CV 2/5] END ........alpha=0.007196856730011514;, score=0.923 total time=   0.1s\n",
      "[CV 3/5] END ........alpha=0.007196856730011514;, score=0.929 total time=   0.1s\n",
      "[CV 4/5] END ........alpha=0.007196856730011514;, score=0.927 total time=   0.1s\n",
      "[CV 5/5] END ........alpha=0.007196856730011514;, score=0.926 total time=   0.1s\n",
      "[CV 1/5] END ........alpha=0.012067926406393264;, score=0.925 total time=   0.1s\n",
      "[CV 2/5] END ........alpha=0.012067926406393264;, score=0.925 total time=   0.1s\n",
      "[CV 3/5] END ........alpha=0.012067926406393264;, score=0.930 total time=   0.1s\n",
      "[CV 4/5] END ........alpha=0.012067926406393264;, score=0.928 total time=   0.1s\n",
      "[CV 5/5] END ........alpha=0.012067926406393264;, score=0.927 total time=   0.1s\n",
      "[CV 1/5] END ........alpha=0.020235896477251554;, score=0.927 total time=   0.1s\n",
      "[CV 2/5] END ........alpha=0.020235896477251554;, score=0.926 total time=   0.1s\n",
      "[CV 3/5] END ........alpha=0.020235896477251554;, score=0.932 total time=   0.1s\n",
      "[CV 4/5] END ........alpha=0.020235896477251554;, score=0.928 total time=   0.1s\n",
      "[CV 5/5] END ........alpha=0.020235896477251554;, score=0.928 total time=   0.1s\n",
      "[CV 1/5] END ..........alpha=0.0339322177189533;, score=0.927 total time=   0.1s\n",
      "[CV 2/5] END ..........alpha=0.0339322177189533;, score=0.927 total time=   0.1s\n",
      "[CV 3/5] END ..........alpha=0.0339322177189533;, score=0.932 total time=   0.1s\n",
      "[CV 4/5] END ..........alpha=0.0339322177189533;, score=0.929 total time=   0.1s\n",
      "[CV 5/5] END ..........alpha=0.0339322177189533;, score=0.929 total time=   0.1s\n",
      "[CV 1/5] END .........alpha=0.05689866029018305;, score=0.928 total time=   0.1s\n",
      "[CV 2/5] END .........alpha=0.05689866029018305;, score=0.926 total time=   0.1s\n",
      "[CV 3/5] END .........alpha=0.05689866029018305;, score=0.932 total time=   0.1s\n",
      "[CV 4/5] END .........alpha=0.05689866029018305;, score=0.929 total time=   0.1s\n",
      "[CV 5/5] END .........alpha=0.05689866029018305;, score=0.930 total time=   0.1s\n",
      "[CV 1/5] END .........alpha=0.09540954763499924;, score=0.928 total time=   0.1s\n",
      "[CV 2/5] END .........alpha=0.09540954763499924;, score=0.925 total time=   0.1s\n",
      "[CV 3/5] END .........alpha=0.09540954763499924;, score=0.931 total time=   0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END .........alpha=0.09540954763499924;, score=0.929 total time=   0.1s\n",
      "[CV 5/5] END .........alpha=0.09540954763499924;, score=0.929 total time=   0.1s\n",
      "[CV 1/5] END .........alpha=0.15998587196060574;, score=0.926 total time=   0.1s\n",
      "[CV 2/5] END .........alpha=0.15998587196060574;, score=0.922 total time=   0.1s\n",
      "[CV 3/5] END .........alpha=0.15998587196060574;, score=0.929 total time=   0.1s\n",
      "[CV 4/5] END .........alpha=0.15998587196060574;, score=0.927 total time=   0.1s\n",
      "[CV 5/5] END .........alpha=0.15998587196060574;, score=0.927 total time=   0.1s\n",
      "[CV 1/5] END .........alpha=0.26826957952797276;, score=0.923 total time=   0.1s\n",
      "[CV 2/5] END .........alpha=0.26826957952797276;, score=0.916 total time=   0.1s\n",
      "[CV 3/5] END .........alpha=0.26826957952797276;, score=0.923 total time=   0.1s\n",
      "[CV 4/5] END .........alpha=0.26826957952797276;, score=0.922 total time=   0.1s\n",
      "[CV 5/5] END .........alpha=0.26826957952797276;, score=0.921 total time=   0.1s\n",
      "[CV 1/5] END ..........alpha=0.4498432668969453;, score=0.915 total time=   0.1s\n",
      "[CV 2/5] END ..........alpha=0.4498432668969453;, score=0.904 total time=   0.1s\n",
      "[CV 3/5] END ..........alpha=0.4498432668969453;, score=0.911 total time=   0.1s\n",
      "[CV 4/5] END ..........alpha=0.4498432668969453;, score=0.911 total time=   0.1s\n",
      "[CV 5/5] END ..........alpha=0.4498432668969453;, score=0.909 total time=   0.1s\n",
      "[CV 1/5] END ..........alpha=0.7543120063354607;, score=0.896 total time=   0.1s\n",
      "[CV 2/5] END ..........alpha=0.7543120063354607;, score=0.881 total time=   0.1s\n",
      "[CV 3/5] END ..........alpha=0.7543120063354607;, score=0.887 total time=   0.1s\n",
      "[CV 4/5] END ..........alpha=0.7543120063354607;, score=0.890 total time=   0.1s\n",
      "[CV 5/5] END ..........alpha=0.7543120063354607;, score=0.886 total time=   0.1s\n",
      "[CV 1/5] END ..........alpha=1.2648552168552958;, score=0.864 total time=   0.1s\n",
      "[CV 2/5] END ..........alpha=1.2648552168552958;, score=0.847 total time=   0.1s\n",
      "[CV 3/5] END ..........alpha=1.2648552168552958;, score=0.853 total time=   0.1s\n",
      "[CV 4/5] END ..........alpha=1.2648552168552958;, score=0.857 total time=   0.1s\n",
      "[CV 5/5] END ..........alpha=1.2648552168552958;, score=0.851 total time=   0.1s\n",
      "[CV 1/5] END ..........alpha=2.1209508879201926;, score=0.824 total time=   0.1s\n",
      "[CV 2/5] END ..........alpha=2.1209508879201926;, score=0.813 total time=   0.1s\n",
      "[CV 3/5] END ..........alpha=2.1209508879201926;, score=0.815 total time=   0.1s\n",
      "[CV 4/5] END ..........alpha=2.1209508879201926;, score=0.818 total time=   0.1s\n",
      "[CV 5/5] END ..........alpha=2.1209508879201926;, score=0.814 total time=   0.1s\n",
      "[CV 1/5] END ...........alpha=3.556480306223136;, score=0.797 total time=   0.1s\n",
      "[CV 2/5] END ...........alpha=3.556480306223136;, score=0.795 total time=   0.1s\n",
      "[CV 3/5] END ...........alpha=3.556480306223136;, score=0.793 total time=   0.1s\n",
      "[CV 4/5] END ...........alpha=3.556480306223136;, score=0.796 total time=   0.1s\n",
      "[CV 5/5] END ...........alpha=3.556480306223136;, score=0.794 total time=   0.1s\n",
      "[CV 1/5] END ...........alpha=5.963623316594637;, score=0.787 total time=   0.1s\n",
      "[CV 2/5] END ...........alpha=5.963623316594637;, score=0.788 total time=   0.1s\n",
      "[CV 3/5] END ...........alpha=5.963623316594637;, score=0.787 total time=   0.1s\n",
      "[CV 4/5] END ...........alpha=5.963623316594637;, score=0.788 total time=   0.1s\n",
      "[CV 5/5] END ...........alpha=5.963623316594637;, score=0.787 total time=   0.1s\n",
      "[CV 1/5] END ........................alpha=10.0;, score=0.786 total time=   0.1s\n",
      "[CV 2/5] END ........................alpha=10.0;, score=0.786 total time=   0.1s\n",
      "[CV 3/5] END ........................alpha=10.0;, score=0.785 total time=   0.1s\n",
      "[CV 4/5] END ........................alpha=10.0;, score=0.786 total time=   0.1s\n",
      "[CV 5/5] END ........................alpha=10.0;, score=0.786 total time=   0.1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=MultinomialNB(),\n",
       "             param_grid={'alpha': array([1.00000000e-10, 1.67683294e-10, 2.81176870e-10, 4.71486636e-10,\n",
       "       7.90604321e-10, 1.32571137e-09, 2.22299648e-09, 3.72759372e-09,\n",
       "       6.25055193e-09, 1.04811313e-08, 1.75751062e-08, 2.94705170e-08,\n",
       "       4.94171336e-08, 8.28642773e-08, 1.38949549e-07, 2.32995181e-07,\n",
       "       3.90693994e-07, 6.55128557e-07, 1.09854114e-06...\n",
       "       2.44205309e-05, 4.09491506e-05, 6.86648845e-05, 1.15139540e-04,\n",
       "       1.93069773e-04, 3.23745754e-04, 5.42867544e-04, 9.10298178e-04,\n",
       "       1.52641797e-03, 2.55954792e-03, 4.29193426e-03, 7.19685673e-03,\n",
       "       1.20679264e-02, 2.02358965e-02, 3.39322177e-02, 5.68986603e-02,\n",
       "       9.54095476e-02, 1.59985872e-01, 2.68269580e-01, 4.49843267e-01,\n",
       "       7.54312006e-01, 1.26485522e+00, 2.12095089e+00, 3.55648031e+00,\n",
       "       5.96362332e+00, 1.00000000e+01])},\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "n_alphas = 50\n",
    "alphas = np.logspace(-10, 1, n_alphas)\n",
    "\n",
    "parameters = {'alpha': alphas}\n",
    "multinomial_nb_ = MultinomialNB()\n",
    "m_nb_grid = GridSearchCV(multinomial_nb_, parameters, verbose=3)\n",
    "\n",
    "m_nb_grid.fit(kk, polar_stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.05689866029018305}"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_nb_grid.best_params_\n",
    "# m_nb_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_nb = m_nb_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity_score = m_nb.feature_log_prob_[1]/m_nb.feature_log_prob_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "polar_words = []\n",
    "\n",
    "for i in m_nb.feature_log_prob_:\n",
    "    words = list(pd.DataFrame({'word':tfidf.get_feature_names_out(), 'prob': i }).sort_values('prob', ascending=False)['word'])\n",
    "    polar_words.extend(words[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_polarity_data = pd.DataFrame({'word':tfidf.get_feature_names_out(), 'prob': polarity_score }).sort_values('prob', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "polar_words =list(word_polarity_data[-25:]['word']) + list(word_polarity_data[:25]['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['favorite',\n",
       " 'yum',\n",
       " 'delish',\n",
       " 'yummm',\n",
       " 'love',\n",
       " 'divine',\n",
       " 'montagu',\n",
       " 'impeccable',\n",
       " 'deliciousness',\n",
       " 'great',\n",
       " 'mouthwatering',\n",
       " 'gem',\n",
       " 'perfect',\n",
       " 'yummy',\n",
       " 'mazing',\n",
       " 'unpretentious',\n",
       " 'troy',\n",
       " 'textures',\n",
       " 'excellent',\n",
       " 'awesome',\n",
       " 'fantastic',\n",
       " 'awsome',\n",
       " 'amazing',\n",
       " 'perfection',\n",
       " 'delicious',\n",
       " 'worst',\n",
       " 'unacceptable',\n",
       " 'crooks',\n",
       " 'aweful',\n",
       " 'horrible',\n",
       " 'fraud',\n",
       " 'unprofessional',\n",
       " 'rude',\n",
       " 'blamed',\n",
       " 'disgrace',\n",
       " 'unhelpful',\n",
       " 'incompetent',\n",
       " 'terrible',\n",
       " 'awful',\n",
       " 'livid',\n",
       " 'rudest',\n",
       " 'rudely',\n",
       " 'poisoning',\n",
       " 'disgusting',\n",
       " 'unethical',\n",
       " 'tasteless',\n",
       " 'disrespectful',\n",
       " 'harassing',\n",
       " 'refund',\n",
       " 'uncalled']"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "for word in polar_words:\n",
    "    d[word] = sum([word in i for i in polar_texts ])/len(polar_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>montagu</td>\n",
       "      <td>0.000172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>harassing</td>\n",
       "      <td>0.000180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>uncalled</td>\n",
       "      <td>0.000197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>aweful</td>\n",
       "      <td>0.000197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>livid</td>\n",
       "      <td>0.000283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>crooks</td>\n",
       "      <td>0.000360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>unethical</td>\n",
       "      <td>0.000360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>disgrace</td>\n",
       "      <td>0.000395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>awsome</td>\n",
       "      <td>0.000480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>fraud</td>\n",
       "      <td>0.000583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>rudest</td>\n",
       "      <td>0.000635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mouthwatering</td>\n",
       "      <td>0.000695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>blamed</td>\n",
       "      <td>0.000695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>unpretentious</td>\n",
       "      <td>0.000755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yummm</td>\n",
       "      <td>0.000798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>unhelpful</td>\n",
       "      <td>0.000944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>disrespectful</td>\n",
       "      <td>0.000952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>troy</td>\n",
       "      <td>0.000995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>textures</td>\n",
       "      <td>0.001158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>incompetent</td>\n",
       "      <td>0.001312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>unacceptable</td>\n",
       "      <td>0.001613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>rudely</td>\n",
       "      <td>0.001973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>poisoning</td>\n",
       "      <td>0.002042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>deliciousness</td>\n",
       "      <td>0.002376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>tasteless</td>\n",
       "      <td>0.003500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>divine</td>\n",
       "      <td>0.003526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>unprofessional</td>\n",
       "      <td>0.004238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>impeccable</td>\n",
       "      <td>0.004744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>delish</td>\n",
       "      <td>0.006305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>refund</td>\n",
       "      <td>0.006648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>disgusting</td>\n",
       "      <td>0.006760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>awful</td>\n",
       "      <td>0.010551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>perfection</td>\n",
       "      <td>0.011924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>terrible</td>\n",
       "      <td>0.015149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>horrible</td>\n",
       "      <td>0.018529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>yummy</td>\n",
       "      <td>0.020467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>worst</td>\n",
       "      <td>0.021179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>rude</td>\n",
       "      <td>0.024868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yum</td>\n",
       "      <td>0.027776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gem</td>\n",
       "      <td>0.028102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>fantastic</td>\n",
       "      <td>0.039785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>excellent</td>\n",
       "      <td>0.055475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>awesome</td>\n",
       "      <td>0.079304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>perfect</td>\n",
       "      <td>0.086973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>favorite</td>\n",
       "      <td>0.088878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>delicious</td>\n",
       "      <td>0.102834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>amazing</td>\n",
       "      <td>0.118103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mazing</td>\n",
       "      <td>0.130010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>love</td>\n",
       "      <td>0.191086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>great</td>\n",
       "      <td>0.289382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              word      freq\n",
       "6          montagu  0.000172\n",
       "47       harassing  0.000180\n",
       "49        uncalled  0.000197\n",
       "28          aweful  0.000197\n",
       "39           livid  0.000283\n",
       "27          crooks  0.000360\n",
       "44       unethical  0.000360\n",
       "34        disgrace  0.000395\n",
       "21          awsome  0.000480\n",
       "30           fraud  0.000583\n",
       "40          rudest  0.000635\n",
       "10   mouthwatering  0.000695\n",
       "33          blamed  0.000695\n",
       "15   unpretentious  0.000755\n",
       "3            yummm  0.000798\n",
       "35       unhelpful  0.000944\n",
       "46   disrespectful  0.000952\n",
       "16            troy  0.000995\n",
       "17        textures  0.001158\n",
       "36     incompetent  0.001312\n",
       "26    unacceptable  0.001613\n",
       "41          rudely  0.001973\n",
       "42       poisoning  0.002042\n",
       "8    deliciousness  0.002376\n",
       "45       tasteless  0.003500\n",
       "5           divine  0.003526\n",
       "31  unprofessional  0.004238\n",
       "7       impeccable  0.004744\n",
       "2           delish  0.006305\n",
       "48          refund  0.006648\n",
       "43      disgusting  0.006760\n",
       "38           awful  0.010551\n",
       "23      perfection  0.011924\n",
       "37        terrible  0.015149\n",
       "29        horrible  0.018529\n",
       "13           yummy  0.020467\n",
       "25           worst  0.021179\n",
       "32            rude  0.024868\n",
       "1              yum  0.027776\n",
       "11             gem  0.028102\n",
       "20       fantastic  0.039785\n",
       "18       excellent  0.055475\n",
       "19         awesome  0.079304\n",
       "12         perfect  0.086973\n",
       "0         favorite  0.088878\n",
       "24       delicious  0.102834\n",
       "22         amazing  0.118103\n",
       "14          mazing  0.130010\n",
       "4             love  0.191086\n",
       "9            great  0.289382"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'word':d.keys(), 'freq':d.values()}).sort_values('freq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 4: food_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Look over all reviews of restaurants.  You can determine which businesses are restaurants by looking in the `yelp_train_academic_dataset_business.json.gz` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "with gzip.open('yelp_train_academic_dataset_business.json.gz') as f:\n",
    "    business_data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Each row of this file corresponds to a single business.  The category key gives a list of categories for each; take all where \"Restaurants\" appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Restaurants' in business_data[1]['categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants = [i for i in business_data if 'Restaurants' in i['categories']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'votes': {'funny': 0, 'useful': 0, 'cool': 0},\n",
       " 'user_id': 'Qrs3EICADUKNFoUq2iHStA',\n",
       " 'review_id': '_ePLBPrkrf4bhyiKWEn4Qg',\n",
       " 'stars': 1,\n",
       " 'date': '2013-04-19',\n",
       " 'text': \"I don't know what Dr. Goldberg was like before  moving to Arizona, but let me tell you, STAY AWAY from this doctor and this office. I was going to Dr. Johnson before he left and Goldberg took over when Johnson left. He is not a caring doctor. He is only interested in the co-pay and having you come in for medication refills every month. He will not give refills and could less about patients's financial situations. Trying to get your 90 days mail away pharmacy prescriptions through this guy is a joke. And to make matters even worse, his office staff is incompetent. 90% of the time when you call the office, they'll put you through to a voice mail, that NO ONE ever answers or returns your call. Both my adult children and husband have decided to leave this practice after experiencing such frustration. The entire office has an attitude like they are doing you a favor. Give me a break! Stay away from this doc and the practice. You deserve better and they will not be there when you really need them. I have never felt compelled to write a bad review about anyone until I met this pathetic excuse for a doctor who is all about the money.\",\n",
       " 'type': 'review',\n",
       " 'business_id': 'vcNAWiLM4dR7D2nwwJ7nCA'}"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "restaurant_ids = pd.DataFrame(restaurants)[['business_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the categories to check for spelling and capitalization\n",
    "grader.check(len(restaurant_ids) == 12876)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "The \"business_id\" here is the same as in the review data.  Use this to extract the review text for all reviews of restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_reviews = restaurant_ids.merge(pd.DataFrame(data), on='business_id', how='left')[['business_id','text']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just reviews of restaurants\n",
    "# restaurant_ids is helpful here\n",
    "grader.check(len(restaurant_reviews) == 143361)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We want to find collocations --- that is, bigrams that are \"special\" and appear more often than you'd expect from chance. We can think of the corpus as defining an empirical distribution over all *n*-grams.  We can find word pairs that are unlikely to occur consecutively based on the underlying probability of their words. Mathematically, if $p(w)$ be the probability of a word $w$ and $p(w_1 w_2)$ is the probability of the bigram $w_1 w_2$, then we want to look at word pairs $w_1 w_2$ where the statistic\n",
    "\n",
    "  $$ \\frac{p(w_1 w_2)}{p(w_1) p(w_2)} $$\n",
    "\n",
    "is high.  Return the top 100 (mostly food) bigrams with this statistic with the 'right' prior factor (see below).\n",
    "\n",
    "Estimating the probabilities is simply a matter of counting, and there are number of approaches that will work.  One is to use one of the tokenizers to count up how many times each word and each bigram appears in each review, and then sum those up over all reviews.  You might want to know that the `CountVectorizer` has a `.get_feature_names_out()` method which gives the string associated with each column.  (Question for thought: Why doesn't the `HashingVectorizer` have a similar method?)\n",
    "\n",
    "*Questions:* This statistic is a ratio and problematic when the denominator is small.  We can fix this by applying Bayesian smoothing to $p(w)$ (i.e. mixing the empirical distribution with the uniform distribution over the vocabulary).\n",
    "\n",
    "1. How does changing this smoothing parameter affect the word pairs you get qualitatively?\n",
    "\n",
    "2. We can interpret the smoothing parameter as adding a constant number of occurrences of each word to our distribution.  Does this help you determine set a reasonable value for this 'prior factor'?\n",
    "\n",
    "3. For fun: also check out [Amazon's Statistically Improbable Phrases](http://en.wikipedia.org/wiki/Statistically_Improbable_Phrases).\n",
    "\n",
    "*Implementation note:*\n",
    "As you adjust the size of the Bayesian smoothing parameter, you will notice first nonsense phrases being removed and then legitimate bigrams being removed, leaving you with only generic bigrams.  The goal is to find a value of the smoothing parameter between these two transitions.\n",
    "\n",
    "The reference solution is not an aggressive filterer: it errors in favor of leaving apparently nonsensical words. On further consideration, many of these are actually somewhat meaningful. The smoothing parameter chosen in the reference solution is equivalent to giving each word 30 previous appearances prior to considering this data.  This was chosen by generating a list of bigrams for a range of smoothing parameters and seeing how many of the bigrams were shared between neighboring values.  When the shared fraction reached 95%, we judged the solution to have converged.\n",
    "\n",
    "There are a few reviews that include the same nonsense strings multiple times.  To keep these from showing up in our results, we set `min_df=10`, to ensure that a bigram occurs in at least 10 reviews before we consider it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         If you like lot lizards, you'll love the Pine ...\n",
       "1         Only went here once about a year and a half ag...\n",
       "2         Ate a Saturday morning breakfast at the Pine C...\n",
       "3         This is definitely not your usual truck stop. ...\n",
       "4         I like this location better than the one near ...\n",
       "                                ...                        \n",
       "144941    Barely open less than a week and I've been her...\n",
       "144942    Healthy Food that Keeps this Realtor on the Go...\n",
       "144943    So happy to have this healthy eatery option ri...\n",
       "144945    My new favorite restaurant.  They have 22 diff...\n",
       "144946    GreAt food awesome service . The best fish in ...\n",
       "Name: text, Length: 143361, dtype: object"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restaurant_reviews['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "##count single words\n",
    "cv = CountVectorizer(min_df=10,stop_words='english')\n",
    "single=cv.fit_transform(restaurant_reviews['text'])\n",
    "\n",
    "unigrams=list(cv.get_feature_names_out())\n",
    "counts = single.sum(axis=0).A1\n",
    "\n",
    "freq_distribution = Counter(dict(zip(unigrams, counts)))\n",
    "\n",
    "#count double words\n",
    "cv = CountVectorizer(ngram_range=(2,2),min_df=10,stop_words='english')\n",
    "double = cv.fit_transform(restaurant_reviews['text'])\n",
    "\n",
    "bigrams=list(cv.get_feature_names_out ())\n",
    "counts_bi = double.sum(axis=0).A1\n",
    "\n",
    "freq_distribution_bi = Counter(dict(zip(bigrams, counts_bi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##calculate p(w)\n",
    "value=sum(freq_distribution.values())\n",
    "for item, count in freq_distribution.items():\n",
    "    freq_distribution[item]+=30 \n",
    "    freq_distribution[item]/= value\n",
    "\n",
    "##calculate p(w1w2)\n",
    "value=sum(freq_distribution_bi.values())\n",
    "for item, count in freq_distribution_bi.items():\n",
    "    freq_distribution_bi[item] /= value\n",
    "\n",
    "##calculate ratio\n",
    "for item, count in freq_distribution_bi.items():\n",
    "    lis_grams=item.split()\n",
    "    value1=freq_distribution[lis_grams[0]]\n",
    "    value2=freq_distribution[lis_grams[1]]\n",
    "    value=value1*value2\n",
    "    freq_distribution_bi[item] /= value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "\n",
    "top100=[item[0] for item in freq_distribution_bi.most_common(100)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "nbclean": true,
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
